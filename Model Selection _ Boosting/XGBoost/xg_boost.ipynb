{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xg_boost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauravpks/ml-repo/blob/master/Model%20Selection%20_%20Boosting/XGBoost/xg_boost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s-wY3rgDF49"
      },
      "source": [
        "#Boosting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Mzv3BeDK0n"
      },
      "source": [
        "In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFK0NGASDTiJ"
      },
      "source": [
        "To convert weak learner to strong learner, we’ll combine the prediction of each weak learner using methods like:\r\n",
        "1. Using average/ weighted average\r\n",
        "2. Considering prediction has higher vote\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9yF1saHDlvo"
      },
      "source": [
        "**How Boosting Algorithms works?**\r\n",
        "\r\n",
        "Now we know that, boosting combines weak learner a.k.a. base learner to form a strong rule. An immediate question which should pop in your mind is, ‘How boosting identify weak rules?‘\r\n",
        "\r\n",
        "To find weak rule, we apply base learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\r\n",
        "\r\n",
        "Here’s another question which might haunt you, ‘How do we choose different distribution for each round?’\r\n",
        "\r\n",
        "For choosing the right distribution, here are the following steps:\r\n",
        "\r\n",
        "Step 1:  The base learner takes all the distributions and assign equal weight or attention to each observation.\r\n",
        "\r\n",
        "Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\r\n",
        "\r\n",
        "Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\r\n",
        "\r\n",
        "Finally, it combines the outputs from weak learner and creates  a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FLud1n-3pVm"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO8VPU6n3vES"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clDSsF7P33NU"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGpwK5XD386E"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcksk88u4Ae8"
      },
      "source": [
        "dataset = pd.read_csv('https://raw.githubusercontent.com/gauravpks/ml-repo/master/Model%20Selection%20_%20Boosting/Classification/Data.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNn2RnST6_Q-"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajhBL-er7Gry"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y89ctGZ7Mcx"
      },
      "source": [
        "## Training XGBoost on the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ude1J0E47SKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4441a7b-76f7-4308-9fc9-a26b99fed5b4"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "classifier = XGBClassifier()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivqmubzW7dFJ"
      },
      "source": [
        "## Making the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUSZ3zm_7gRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffbd2093-bdb6-404d-880e-8d514cf384f5"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "y_pred = classifier.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[84  3]\n",
            " [ 0 50]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9781021897810219"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnbCjHgQ8XPn"
      },
      "source": [
        "## Applying k-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYbfiITD8ZAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44cf3ded-8a19-4fcd-acf1-0511647361cb"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
        "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 96.53 %\n",
            "Standard Deviation: 2.07 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}